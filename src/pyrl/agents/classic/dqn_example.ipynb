{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrl.agents.classic import DQNAgent\n",
    "from pyrl.agents.survival import SurvivalDQNAgent\n",
    "import numpy as np\n",
    "# from tensorforce.environments import Environment\n",
    "# from pyrl.environments import CustomEnvironment\n",
    "from itertools import count\n",
    "from pyrl import Sim\n",
    "from pyrl.environments.grid import GridEnv, GridEnvRender\n",
    "from tensorforce.environments import Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_size = (50, 5)\n",
    "num_rows = map_size[1]\n",
    "num_cols = map_size[0]\n",
    "minor_r = 5.0\n",
    "major_r = 100.0\n",
    "reward_targets = {major_r : [(num_cols - 2, num_rows // 2)],\n",
    "                  minor_r : [(3*(num_cols - 1) // 5, num_rows // 2), ((num_cols - 1) // 3, num_rows // 2)]}\n",
    "cell_size = 25\n",
    "horizon = 5000\n",
    "points = 6\n",
    "repeat = 3\n",
    "survival_threshold = 250\n",
    "exploration_threshold = 500\n",
    "gamma = 0.99 # discount factor\n",
    "initial_budgets = np.linspace(100, horizon, points, dtype=int)\n",
    "replay_capacity = 5000\n",
    "batch_size = 256\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_time_mean = np.full(initial_budgets.shape, -1)\n",
    "dqn_exploration_rate = np.full(initial_budgets.shape, -1)\n",
    "dqn_alive_rate = np.full(initial_budgets.shape, -1)\n",
    "dqn_budget_evolutions_mean = np.full(initial_budgets.shape, None)\n",
    "dqn_budget_evolutions_max = np.full(initial_budgets.shape, None)\n",
    "dqn_budget_evolutions_min = np.full(initial_budgets.shape, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation_started_callback(sim, env, agent):\n",
    "    print(\"START SIM\")\n",
    "    sim.metrics = dict(\n",
    "        time = 0,\n",
    "        exploration = np.zeros(np.prod(env.observation_space.nvec) + env.action_space.n),\n",
    "        budget = np.zeros((sim.episode_horizon,), dtype=int)\n",
    "    )\n",
    "\n",
    "def simulation_finished_callback(sim, env, agent):\n",
    "    print(\"END SIM\")\n",
    "\n",
    "def episode_started_callback(sim, env, agent):\n",
    "    print(\"START EPISODE\")\n",
    "\n",
    "def episode_finished_callback(sim, env, agent):\n",
    "    print(\"END EPISODE\")\n",
    "\n",
    "def round_started_callback(sim, env, agent):\n",
    "    pass\n",
    "\n",
    "def round_finished_callback(sim, env, agent):\n",
    "    print(\"END ROUND\")\n",
    "    sim.metrics[\"time\"] = sim.metrics[\"time\"] + 1\n",
    "    state_action_index = tuple(np.concatenate( (agent.get_state(), agent.get_action()) ) )\n",
    "    state_action_index = tuple(agent.get_state_action())\n",
    "    v = sim.metrics[\"exploration\"].item(state_action_index)\n",
    "    sim.metrics[\"exploration\"].itemset(state_action_index, v+1)\n",
    "    sim.metrics[\"budget\"][sim.t-1] = agent.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Classic DQN\n",
      "====> Classic DQN 100 | Try 1\n",
      "TEST CLASSIC DQN AGENT\n",
      "START SIM\n",
      "START EPISODE\n",
      "episode_finished_callback() missing 1 required positional argument: 'agent'\n",
      "END SIM\n",
      "Time mean : 100\n",
      "Alive rate : 0%\n",
      "Exploration rate : 0%\n",
      "====> Classic DQN 100 | Try 2\n",
      "TEST CLASSIC DQN AGENT\n",
      "START SIM\n",
      "START EPISODE\n",
      "episode_finished_callback() missing 1 required positional argument: 'agent'\n",
      "END SIM\n",
      "Time mean : 100\n",
      "Alive rate : 0%\n",
      "Exploration rate : 0%\n",
      "====> Classic DQN 100 | Try 3\n",
      "TEST CLASSIC DQN AGENT\n",
      "START SIM\n",
      "START EPISODE\n",
      "episode_finished_callback() missing 1 required positional argument: 'agent'\n",
      "END SIM\n",
      "Time mean : 100\n",
      "Alive rate : 0%\n",
      "Exploration rate : 0%\n",
      "====> Classic DQN 1080 | Try 1\n",
      "TEST CLASSIC DQN AGENT\n",
      "START SIM\n",
      "START EPISODE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayman\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:928: UserWarning: Using a target size (torch.Size([256, 1])) that is different to the input size (torch.Size([256, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27412\\356444276.py\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0msim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ayman\\desktop\\pyrl\\src\\pyrl\\_base.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, episode_horizon, num_episodes, num_simulations)\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m                             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m                             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m                             \u001b[1;31m# print(agent.Q)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m                             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"time\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"time\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ayman\\desktop\\pyrl\\src\\pyrl\\agents\\survival\\dqn.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dqn_time_mean = np.full(initial_budgets.shape, -1)\n",
    "dqn_exploration_rate = np.full(initial_budgets.shape, -1)\n",
    "dqn_alive_rate = np.full(initial_budgets.shape, -1)\n",
    "dqn_budget_evolutions_mean = np.full(initial_budgets.shape, None)\n",
    "dqn_budget_evolutions_max = np.full(initial_budgets.shape, None)\n",
    "dqn_budget_evolutions_min = np.full(initial_budgets.shape, None)\n",
    "dqn_exploration_map = np.full(initial_budgets.shape, None)\n",
    "\n",
    "env = GridEnv(num_rows=num_rows, num_cols=num_cols, \n",
    "              reward_mode=\"s'\", reward_targets=reward_targets, default_reward=-1.0,\n",
    "              render_mode=\"external\")\n",
    "\n",
    "print(\"====> Classic DQN\")\n",
    "\n",
    "for i, b in enumerate(initial_budgets):\n",
    "    nb_alive = 0\n",
    "    # print(f\"b={b}\", end=\" \")\n",
    "    for j in range(repeat):\n",
    "        print(f\"====> Classic DQN {b} | Try {j + 1}\")\n",
    "        agent_DQN = SurvivalDQNAgent(observation_space=env.observation_space,\n",
    "                        action_space=env.action_space,\n",
    "                        batch_size=batch_size,\n",
    "                        initial_budget=b,\n",
    "                        gamma=gamma,\n",
    "                        learning_rate=learning_rate\n",
    "                        )\n",
    "        window = GridEnvRender(env, agent_DQN, cell_size=cell_size)\n",
    "\n",
    "        env._render_frame = window.refresh\n",
    "\n",
    "        print(\"TEST CLASSIC DQN AGENT\")\n",
    "\n",
    "        sim = Sim(agent_DQN,\n",
    "                env,\n",
    "                episode_horizon=horizon,\n",
    "                num_simulations=1,\n",
    "                rl_config='classic',\n",
    "                simulation_started_callback=simulation_started_callback,\n",
    "                simulation_finished_callback=simulation_finished_callback,\n",
    "                episode_started_callback=episode_started_callback,\n",
    "                episode_finished_callback=episode_finished_callback,\n",
    "                round_started_callback=round_started_callback,\n",
    "                round_finished_callback=round_finished_callback\n",
    "                )\n",
    "\n",
    "        try:\n",
    "            sim.run()\n",
    "        except:\n",
    "            window.close()\n",
    "            raise\n",
    "\n",
    "        if dqn_time_mean[i] == -1:\n",
    "            dqn_time_mean[i] = sim.metrics[\"time\"]\n",
    "        else:\n",
    "            dqn_time_mean[i] = dqn_time_mean[i] + (1/j) * (sim.metrics[\"time\"] - dqn_time_mean[i])\n",
    "        \n",
    "        exploration_rate = (np.count_nonzero(sim.metrics[\"exploration\"]) / (np.prod(env.observation_space.nvec) * env.action_space.n)) * 100\n",
    "\n",
    "        if dqn_exploration_rate[i] == -1:\n",
    "            dqn_exploration_rate[i] = exploration_rate\n",
    "        else:\n",
    "            dqn_exploration_rate[i] = dqn_exploration_rate[i] + (1 / j) * (exploration_rate - dqn_exploration_rate[i])\n",
    "\n",
    "        if agent_DQN.b > 0:\n",
    "            nb_alive = nb_alive + 1\n",
    "\n",
    "        dqn_alive_rate[i] = nb_alive / (j+1) * 100\n",
    "\n",
    "        # budget evolution mean\n",
    "        if dqn_budget_evolutions_mean[i] is None:\n",
    "            dqn_budget_evolutions_mean[i] = sim.metrics[\"budget\"]\n",
    "        else:\n",
    "            dqn_budget_evolutions_mean[i] = dqn_budget_evolutions_mean[i] + (1 / j) * (sim.metrics[\"budget\"] - dqn_budget_evolutions_mean[i])\n",
    "\n",
    "        # budget evolution max\n",
    "        if dqn_budget_evolutions_max[i] is None:\n",
    "            dqn_budget_evolutions_max[i] = sim.metrics[\"budget\"]\n",
    "        else:\n",
    "            dqn_budget_evolutions_max[i] = np.maximum(dqn_budget_evolutions_max[i], sim.metrics[\"budget\"])\n",
    "\n",
    "        # budget evolution min\n",
    "        if dqn_budget_evolutions_min[i] is None:\n",
    "            dqn_budget_evolutions_min[i] = sim.metrics[\"budget\"]\n",
    "        else:\n",
    "            dqn_budget_evolutions_min[i] = np.minimum(dqn_budget_evolutions_min[i], sim.metrics[\"budget\"])\n",
    "        \n",
    "        # exploration map\n",
    "        if dqn_exploration_map[i] is None:\n",
    "            dqn_exploration_map[i] = sim.metrics[\"exploration\"]\n",
    "        else:\n",
    "            dqn_exploration_map[i] = dqn_exploration_map[i] + (1 / j) * (sim.metrics[\"exploration\"] - dqn_exploration_map[i])\n",
    "\n",
    "        print(f\"Time mean : {dqn_time_mean[i]}\")\n",
    "        print(f\"Alive rate : {dqn_alive_rate[i]}%\")\n",
    "        print(f\"Exploration rate : {dqn_exploration_rate[i]}%\")\n",
    "\n",
    "window.close()\n",
    "\n",
    "dqn_1_time_mean = dqn_time_mean\n",
    "dqn_1_exploration_rate = dqn_exploration_rate\n",
    "dqn_1_alive_rate = dqn_alive_rate\n",
    "dqn_1_budget_evolutions_mean = dqn_budget_evolutions_mean\n",
    "dqn_1_budget_evolutions_max = dqn_budget_evolutions_max\n",
    "dqn_1_budget_evolutions_min = dqn_budget_evolutions_min\n",
    "dqn_1_exploration_map = dqn_exploration_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 5, 4)\n",
      "(50, 5)\n",
      "[ 5 50]\n",
      "TEST CLASSIC DQN AGENT\n",
      "250\n",
      "[0 2]\n",
      "START SIM\n",
      "START EPISODE\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13876\\539190525.py\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0msim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ayman\\desktop\\pyrl\\src\\pyrl\\_base.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, episode_horizon, num_episodes, num_simulations)\u001b[0m\n\u001b[0;32m    589\u001b[0m                                 \u001b[1;31m# print(\"Q-values=\\n\", agent.Q)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m                                 \u001b[1;31m# print(agent.observation_shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m                             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m                             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ayman\\desktop\\pyrl\\src\\pyrl\\environments\\grid.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;31m#old_agent_location = self._get_obs()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ayman\\desktop\\pyrl\\src\\pyrl\\_base.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender_mode\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterrupted\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ayman\\desktop\\pyrl\\src\\pyrl\\environments\\grid.py\u001b[0m in \u001b[0;36mrefresh\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[1;31m#refresh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[0mpg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "env = GridEnv(num_rows=num_rows, num_cols=num_cols, \n",
    "              reward_mode=\"s'\", reward_targets=reward_targets, default_reward=-1.0,\n",
    "              render_mode=\"external\")\n",
    "Q = np.random.sample(env.observation_shape + env.action_shape)\n",
    "print(Q.shape)\n",
    "print(env.observation_shape)\n",
    "\n",
    "print(env.observation_space.nvec)\n",
    "for b in initial_budgets:\n",
    "    agent_DQN = DQNAgent(observation_space=env.observation_space,\n",
    "                        action_space=env.action_space,\n",
    "                        batch_size=batch_size,\n",
    "                        initial_budget=b,\n",
    "                        gamma=gamma,\n",
    "                        learning_rate=learning_rate\n",
    "                        )\n",
    "\n",
    "    window = GridEnvRender(env, agent_DQN, cell_size=cell_size)\n",
    "\n",
    "    env._render_frame = window.refresh\n",
    "\n",
    "    print(\"TEST CLASSIC DQN AGENT\")\n",
    "\n",
    "    sim = Sim(agent_DQN,\n",
    "            env,\n",
    "            episode_horizon=horizon,\n",
    "            num_simulations=repeat,\n",
    "            rl_config='classic',\n",
    "            simulation_started_callback=simulation_started_callback,\n",
    "            simulation_finished_callback=simulation_finished_callback,\n",
    "            episode_started_callback=episode_started_callback,\n",
    "            episode_finished_callback=episode_finished_callback,\n",
    "            round_started_callback=round_started_callback,\n",
    "            round_finished_callback=round_finished_callback\n",
    "            )\n",
    "\n",
    "    try:\n",
    "        sim.run()\n",
    "    except:\n",
    "        window.close()\n",
    "        raise\n",
    "\n",
    "window.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
