{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test des librairies d'apprentissage par renforcement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MDPToolBox\n",
    "\n",
    "https://pymdptoolbox.readthedocs.io/en/latest/\n",
    "\n",
    "### Avantages\n",
    "\n",
    "- Simple d'utilisation\n",
    "- Personnalisation du MDP\n",
    "\n",
    "### Inconvénients\n",
    "\n",
    "- Ne fonctionne pas si on ne connait pas la dynamique de l'environnement\n",
    "- Pas d'optimisation\n",
    "\n",
    "### Algorithmes disponibles\n",
    "\n",
    "- Finite horizon\n",
    "- Policy Iteration\n",
    "- Policy Iteration Modified\n",
    "- QLearning\n",
    "- Relative Value Iteration\n",
    "- Value Iteration\n",
    "- Value Iteration GS\n",
    "\n",
    "### Intégration, modification, confiance\n",
    "\n",
    " - Simple à intégrer dans le module\n",
    " - Simple à modifier, presque aucune dépendance externe\n",
    " - Dernier commit 2015\n",
    " - Créé par Steven Cordwell, doctorant à l'université de Lincoln\n",
    " - Bonne documentation\n",
    "\n",
    "### Exemple d'utilisation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[68.83586282, 41.47533664],\n",
       "       [72.57438227, 40.88926026],\n",
       "       [77.01371452, 65.09179593]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mdptoolbox, mdptoolbox.example\n",
    "\n",
    "P, R = mdptoolbox.example.forest()\n",
    "\n",
    "agent = mdptoolbox.mdp.QLearning(P, R, 0.96)\n",
    "\n",
    "agent.run()\n",
    "\n",
    "agent.Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyQLearning\n",
    "\n",
    "https://code.accel-brain.com/Reinforcement-Learning/\n",
    "\n",
    "### Avantages\n",
    "\n",
    "### Inconvénients\n",
    "\n",
    "- Code difficilement lisible\n",
    "- Peu d'options par défaut\n",
    "- Dépendance à mxnet\n",
    "\n",
    "### Algorithmes disponibles\n",
    "\n",
    "- EpsilonGreedyQlearning\n",
    "- BoltzmanQLearning\n",
    "\n",
    "### Intégration, modification, confiance\n",
    "\n",
    "- Réalisé par Accel Brain (Société japonaise)\n",
    "- Documentation peu lisible\n",
    "- Dernier commit il y a 1 mois\n",
    "\n",
    "### Exemple d'utilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\fperotto\\AppData\\Roaming\\Python\\Python38\\site-packages\\mxnet\\numpy\\utils.py:37: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  bool = onp.bool\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyqlearning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msamplabledata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicysampler\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_mxnet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmaze_policy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MazePolicy\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelbrainbase\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomputableloss\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_mxnet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01ml2_norm_loss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m L2NormLoss\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelbrainbase\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrollablemodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_mxnet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdqlcontroller\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdqn_controller\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DQNController\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\test_melvine\\lib\\site-packages\\pyqlearning\\samplabledata\\policysampler\\_mxnet\\maze_policy.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelbrainbase\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msamplabledata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy_sampler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PolicySampler\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmxnet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndarray\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmxnet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmx\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\mxnet\\__init__.py:33\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# version info\u001b[39;00m\n\u001b[0;32m     31\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39m__version__\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m contrib\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ndarray\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ndarray \u001b[38;5;28;01mas\u001b[39;00m nd\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\mxnet\\contrib\\__init__.py:30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograd\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensorboard\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m text\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m onnx\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m io\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\mxnet\\contrib\\text\\__init__.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vocab\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m embedding\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\mxnet\\contrib\\text\\embedding.py:36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_np_array\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m numpy \u001b[38;5;28;01mas\u001b[39;00m _mx_np\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m numpy_extension \u001b[38;5;28;01mas\u001b[39;00m _mx_npx\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mregister\u001b[39m(embedding_cls):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\mxnet\\numpy\\__init__.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiarray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _op\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _register\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\mxnet\\numpy\\multiarray.py:47\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _internal \u001b[38;5;28;01mas\u001b[39;00m _npi\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndarray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _storage_type, from_numpy\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _get_np_op\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# pylint: disable=wildcard-import,unused-wildcard-import\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fallback\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\mxnet\\numpy\\utils.py:37\u001b[0m\n\u001b[0;32m     35\u001b[0m int64 \u001b[38;5;241m=\u001b[39m onp\u001b[38;5;241m.\u001b[39mint64\n\u001b[0;32m     36\u001b[0m bool_ \u001b[38;5;241m=\u001b[39m onp\u001b[38;5;241m.\u001b[39mbool_\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43monp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m\n\u001b[0;32m     39\u001b[0m pi \u001b[38;5;241m=\u001b[39m onp\u001b[38;5;241m.\u001b[39mpi\n\u001b[0;32m     40\u001b[0m inf \u001b[38;5;241m=\u001b[39m onp\u001b[38;5;241m.\u001b[39minf\n",
      "File \u001b[1;32mC:\\Anaconda3\\envs\\test_melvine\\lib\\site-packages\\numpy\\__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    300\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;66;03m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;66;03m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;66;03m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "from pyqlearning.samplabledata.policysampler._mxnet.maze_policy import MazePolicy\n",
    "from accelbrainbase.computableloss._mxnet.l2_norm_loss import L2NormLoss\n",
    "from accelbrainbase.controllablemodel._mxnet.dqlcontroller.dqn_controller import DQNController\n",
    "from accelbrainbase.observabledata._mxnet.functionapproximator.function_approximator import FunctionApproximator\n",
    "from accelbrainbase.noiseabledata._mxnet.gauss_noise import GaussNoise\n",
    "from accelbrainbase.observabledata._mxnet.neural_networks import NeuralNetworks\n",
    "from accelbrainbase.observabledata._mxnet.convolutional_neural_networks import ConvolutionalNeuralNetworks\n",
    "from accelbrainbase.observabledata._mxnet.convolutionalneuralnetworks.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import mxnet.ndarray as nd\n",
    "import mxnet as mx\n",
    "import mxnet.ndarray as nd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mxnet.gluon.nn import Conv2D\n",
    "from mxnet.gluon.nn import BatchNorm\n",
    "from mxnet import MXNetError\n",
    "from logging import getLogger\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from logging import getLogger, StreamHandler, NullHandler, DEBUG, ERROR\n",
    "\n",
    "logger = getLogger(\"accelbrainbase\")\n",
    "handler = StreamHandler()\n",
    "handler.setLevel(DEBUG)\n",
    "logger.setLevel(DEBUG)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "policy_sampler = MazePolicy(\n",
    "    batch_size=25,\n",
    "    map_size=(20, 20), \n",
    "    moving_max_dist=3,\n",
    "    memory_num=8,\n",
    "    possible_n=5,\n",
    "    repeating_penalty=0.5,\n",
    "    ctx=mx.gpu(),\n",
    ")\n",
    "\n",
    "computable_loss = L2NormLoss()\n",
    "ctx = mx.gpu()\n",
    "\n",
    "output_nn = NeuralNetworks(\n",
    "    # is-a `ComputableLoss` or `mxnet.gluon.loss`.\n",
    "    computable_loss=computable_loss,\n",
    "    # `list` of int` of the number of units in hidden/output layers.\n",
    "    units_list=[1],\n",
    "    # `list` of act_type` in `mxnet.ndarray.Activation` or `mxnet.symbol.Activation` in input gate.\n",
    "    activation_list=[\"sigmoid\"],\n",
    "    # `list` of `float` of dropout rate.\n",
    "    dropout_rate_list=[0.0],\n",
    "    # `list` of `mxnet.gluon.nn.BatchNorm`.\n",
    "    hidden_batch_norm_list=[None],\n",
    "    # `bool` for using bias or not in output layer(last hidden layer).\n",
    "    output_no_bias_flag=True,\n",
    "    # `bool` for using bias or not in all layer.\n",
    "    all_no_bias_flag=True,\n",
    "    # Call `mxnet.gluon.HybridBlock.hybridize()` or not.\n",
    "    hybridize_flag=True,\n",
    "    # `mx.gpu()` or `mx.cpu()`.\n",
    "    ctx=ctx,\n",
    ")\n",
    "\n",
    "cnn = MobileNetV2(\n",
    "    # is-a `ComputableLoss` or `mxnet.gluon.loss`.\n",
    "    computable_loss=computable_loss,\n",
    "    # is-a `mxnet.initializer.Initializer` for parameters of model. If `None`, it is drawing from the Xavier distribution.\n",
    "    initializer=None,\n",
    "    # `int` of the number of filters in input lauer.\n",
    "    input_filter_n=32,\n",
    "    # `tuple` or `int` of kernel size in input layer.\n",
    "    input_kernel_size=(3, 3),\n",
    "    # `tuple` or `int` of strides in input layer.\n",
    "    input_strides=(1, 1),\n",
    "    # `tuple` or `int` of zero-padding in input layer.\n",
    "    input_padding=(1, 1),\n",
    "    # `list` of information of bottleneck layers whose `dict` means ...\n",
    "    # - `filter_rate`: `float` of filter expfilter.\n",
    "    # - `filter_n`: `int` of the number of filters.\n",
    "    # - `block_n`: `int` of the number of blocks.\n",
    "    # - `stride`: `int` or `tuple` of strides.\n",
    "    bottleneck_dict_list=[\n",
    "        {\n",
    "            \"filter_rate\": 1,\n",
    "            \"filter_n\": 32,\n",
    "            \"block_n\": 1,\n",
    "            \"stride\": 1\n",
    "        },\n",
    "        {\n",
    "            \"filter_rate\": 1,\n",
    "            \"filter_n\": 32,\n",
    "            \"block_n\": 2,\n",
    "            \"stride\": 1\n",
    "        },\n",
    "    ],\n",
    "    # `int` of the number of filters in hidden layers.\n",
    "    hidden_filter_n=64,\n",
    "    # `tuple` or `int` of pooling size in hidden layer.\n",
    "    # If `None`, the pooling layer will not attatched in hidden layer.\n",
    "    pool_size=None,\n",
    "    # is-a `NeuralNetworks` or `mxnet.gluon.block.hybridblock.HybridBlock`.\n",
    "    output_nn=output_nn,\n",
    "    # `str` of name of optimizer.\n",
    "    optimizer_name=\"sgd\",\n",
    "    # Call `mxnet.gluon.HybridBlock.hybridize()` or not.\n",
    "    hybridize_flag=True,\n",
    "    # `mx.gpu()` or `mx.cpu()`.\n",
    "    ctx=ctx,\n",
    ")\n",
    "\n",
    "function_approximator = FunctionApproximator(\n",
    "    model=cnn, \n",
    "    initializer=None,\n",
    "    hybridize_flag=True,\n",
    "    scale=1.0, \n",
    "    ctx=ctx, \n",
    ")\n",
    "\n",
    "DQN = DQNController(\n",
    "    function_approximator=function_approximator,\n",
    "    policy_sampler=policy_sampler,\n",
    "    computable_loss=L2NormLoss(),\n",
    "    optimizer_name=\"SGD\",\n",
    "    learning_rate=1e-06,\n",
    "    hybridize_flag=True,\n",
    "    scale=1.0,\n",
    "    ctx=ctx,\n",
    "    initializer=None,\n",
    "    recursive_learning_flag=False,\n",
    ")\n",
    "DQN.alpha_value = 0.3\n",
    "\n",
    "_ = plt.figure(figsize=(10, 10))\n",
    "plt.imshow(DQN.policy_sampler.map_arr, cmap=\"gray\")\n",
    "plt.tick_params(labelbottom=\"off\",bottom=\"off\")\n",
    "plt.tick_params(labelleft=\"off\",left=\"off\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Execute learning.\n",
    "DQN.learn(\n",
    "    # The number of searching.\n",
    "    iter_n=500,\n",
    ")\n",
    "\n",
    "def moving_avg(arr, window=50):\n",
    "    return np.convolve(arr, np.ones(window) / window, mode='same')[window:-window]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(moving_avg(np.abs(DQN.q_logs_arr[:, 0] - DQN.q_logs_arr[:, 1])), label=\"Loss of Q-Values\", color=\"red\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Acme\n",
    "\n",
    "![Acme logo](https://raw.githubusercontent.com/deepmind/acme/master/docs/imgs/acme.png \"Acme logo\")\n",
    "\n",
    "https://github.com/deepmind/acme\n",
    "\n",
    "https://dm-acme.readthedocs.io/en/latest/index.html\n",
    "\n",
    "### Avantages\n",
    "\n",
    "### Inconvénients\n",
    "\n",
    "### Algorithmes disponibles\n",
    "\n",
    "- Grande base d'algorithmes (https://dm-acme.readthedocs.io/en/latest/user/agents.html)\n",
    "\n",
    "### Intégration, modification, confiance\n",
    "\n",
    "- Construit à partir de Tensorflow\n",
    "- Développé par Deepmind\n",
    "- Documentation pauvre\n",
    "- Dernier commit il y a 1 mois environ\n",
    "\n",
    "### Exemple d'utilisation\n",
    "\n",
    "https://colab.research.google.com/github/deepmind/acme/blob/master/examples/quickstart.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dm_control'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdm_control\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m suite \u001b[38;5;28;01mas\u001b[39;00m dm_suite\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdm_env\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dm_control'"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import collections\n",
    "from dm_control import suite as dm_suite\n",
    "import dm_env\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from acme import specs\n",
    "from acme import wrappers\n",
    "from acme.agents.jax import d4pg\n",
    "from acme.jax import experiments\n",
    "from acme.utils import loggers\n",
    "\n",
    "def make_environment(seed: int) -> dm_env.Environment:\n",
    "    environment = dm_suite.load('cartpole', 'balance')\n",
    "\n",
    "    # Make the observations be a flat vector of all concatenated features.\n",
    "    environment = wrappers.ConcatObservationWrapper(environment)\n",
    "\n",
    "    # Wrap the environment so the expected continuous action spec is [-1, 1].\n",
    "    # Note: this is a no-op on 'control' tasks.\n",
    "    environment = wrappers.CanonicalSpecWrapper(environment, clip=True)\n",
    "\n",
    "    # Make sure the environment outputs single-precision floats.\n",
    "    environment = wrappers.SinglePrecisionWrapper(environment)\n",
    "\n",
    "    return environment\n",
    "\n",
    "def network_factory(spec: specs.EnvironmentSpec) -> d4pg.D4PGNetworks:\n",
    "    return d4pg.make_networks(\n",
    "        spec,\n",
    "        # These correspond to sizes of the hidden layers of an MLP.\n",
    "        policy_layer_sizes=(256, 256),\n",
    "        critic_layer_sizes=(256, 256),\n",
    "    )\n",
    "\n",
    "logger_dict = collections.defaultdict(loggers.InMemoryLogger)\n",
    "def logger_factory(\n",
    "        name: str,\n",
    "        steps_key: Optional[str] = None,\n",
    "        task_id: Optional[int] = None,\n",
    "    ) -> loggers.Logger:\n",
    "    del steps_key, task_id\n",
    "    return logger_dict[name]\n",
    "\n",
    "experiment_config = experiments.ExperimentConfig(\n",
    "    builder=d4pg_builder,\n",
    "    environment_factory=make_environment,\n",
    "    network_factory=network_factory,\n",
    "    logger_factory=logger_factory,\n",
    "    seed=0,\n",
    "    max_num_actor_steps=50_000)  # Each episode is 1000 steps.\n",
    "\n",
    "experiments.run_experiment(\n",
    "    experiment=experiment_config,\n",
    "    eval_every=1000,\n",
    "    num_eval_episodes=1)\n",
    "\n",
    "%matplotlib inline\n",
    "df = pd.DataFrame(logger_dict['evaluator'].data)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.title('Training episodes returns')\n",
    "plt.xlabel('Training episodes')\n",
    "plt.ylabel('Episode return')\n",
    "plt.plot(df['actor_episodes'], df['episode_return'], label='Training Episodes return')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dopamine\n",
    "\n",
    "![Dopamine Logo](https://google.github.io/dopamine/images/dopamine_logo.png \"Dopamine logo\")\n",
    "\n",
    "https://google.github.io/dopamine/docs/\n",
    "\n",
    "### Avantages\n",
    "\n",
    "- Dopamine est conçu pour être modifié directement depuis les sources\n",
    "\n",
    "### Inconvénients\n",
    "\n",
    "- Gin configuration framework\n",
    "\n",
    "### Algorithmes disponibles\n",
    "\n",
    "- C51,\n",
    "- DQN,\n",
    "- IQN,\n",
    "- Quantile (JAX),\n",
    "- Rainbow\n",
    "\n",
    "### Intégration, modification, confiance\n",
    "\n",
    "- Construit à partir de Tensorflow\n",
    "- Développé par Google\n",
    "- Documentation pauvre\n",
    "- Dernier commit il y a 2 mois environ\n",
    "\n",
    "### Exemple d'utilisation\n",
    "\n",
    "https://colab.research.google.com/github/google/dopamine/blob/master/dopamine/colab/cartpole.ipynb#scrollTo=bidurBV0djGi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from dopamine.discrete_domains import run_experiment\n",
    "from dopamine.colab import utils as colab_utils\n",
    "from absl import flags\n",
    "import gin.tf\n",
    "\n",
    "BASE_PATH = '/tmp/colab_dopamine_run'  # @param\n",
    "\n",
    "DQN_PATH = os.path.join(BASE_PATH, 'dqn')\n",
    "\n",
    "dqn_config = \"\"\"\n",
    "# Hyperparameters for a simple DQN-style Cartpole agent. The hyperparameters\n",
    "# chosen achieve reasonable performance.\n",
    "import dopamine.discrete_domains.gym_lib\n",
    "import dopamine.discrete_domains.run_experiment\n",
    "import dopamine.agents.dqn.dqn_agent\n",
    "import dopamine.replay_memory.circular_replay_buffer\n",
    "import gin.tf.external_configurables\n",
    "\n",
    "DQNAgent.observation_shape = %gym_lib.CARTPOLE_OBSERVATION_SHAPE\n",
    "DQNAgent.observation_dtype = %gym_lib.CARTPOLE_OBSERVATION_DTYPE\n",
    "DQNAgent.stack_size = %gym_lib.CARTPOLE_STACK_SIZE\n",
    "DQNAgent.network = @gym_lib.CartpoleDQNNetwork\n",
    "DQNAgent.gamma = 0.99\n",
    "DQNAgent.update_horizon = 1\n",
    "DQNAgent.min_replay_history = 500\n",
    "DQNAgent.update_period = 4\n",
    "DQNAgent.target_update_period = 100\n",
    "DQNAgent.epsilon_fn = @dqn_agent.identity_epsilon\n",
    "DQNAgent.tf_device = '/gpu:0'  # use '/cpu:*' for non-GPU version\n",
    "DQNAgent.optimizer = @tf.train.AdamOptimizer()\n",
    "\n",
    "tf.train.AdamOptimizer.learning_rate = 0.001\n",
    "tf.train.AdamOptimizer.epsilon = 0.0003125\n",
    "\n",
    "create_gym_environment.environment_name = 'CartPole'\n",
    "create_gym_environment.version = 'v0'\n",
    "create_agent.agent_name = 'dqn'\n",
    "TrainRunner.create_environment_fn = @gym_lib.create_gym_environment\n",
    "Runner.num_iterations = 50\n",
    "Runner.training_steps = 1000\n",
    "Runner.evaluation_steps = 1000\n",
    "Runner.max_steps_per_episode = 200  # Default max episode length.\n",
    "\n",
    "WrappedReplayBuffer.replay_capacity = 50000\n",
    "WrappedReplayBuffer.batch_size = 128\n",
    "\"\"\"\n",
    "\n",
    "gin.parse_config(dqn_config, skip_unknown=False)\n",
    "\n",
    "dqn_runner = run_experiment.create_runner(DQN_PATH, schedule='continuous_train')\n",
    "print('Will train DQN agent, please be patient, may be a while...')\n",
    "dqn_runner.run_experiment()\n",
    "print('Done training!')\n",
    "\n",
    "# @title Load the training logs.\n",
    "data = colab_utils.read_experiment(DQN_PATH, verbose=True, summary_keys=['train_episode_returns'])\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "sns.lineplot(x='iteration', y='train_episode_returns', hue='agent',\n",
    "             data=data, ax=ax)\n",
    "plt.title('Cartpole')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RLlib\n",
    "\n",
    "![Ray logo](./assets/ray_logo_w.svg \"Ray logo\")\n",
    "\n",
    "https://docs.ray.io/en/latest/rllib/index.html\n",
    "\n",
    "### Avantages\n",
    "\n",
    "- Code concis et lisible\n",
    "\n",
    "### Inconvénients\n",
    "\n",
    "- Orienté industrie\n",
    "\n",
    "### Algorithmes disponibles\n",
    "\n",
    "- Nombreux algorithmes (https://docs.ray.io/en/latest/rllib/rllib-algorithms.html)\n",
    "\n",
    "### Intégration, modification, confiance\n",
    "\n",
    "- Construit à partir de Tensorflow et Pytorch\n",
    "- Développé par Ray (société)\n",
    "- Documentation bien écrite\n",
    "- Dernier commit il y a 1 semaine environ\n",
    "\n",
    "### Exemple d'utilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "config = (  # 1. Configure the algorithm,\n",
    "    PPOConfig()\n",
    "    .environment(\"Taxi-v3\")\n",
    "    .rollouts(num_rollout_workers=2)\n",
    "    .framework(\"tf2\")\n",
    "    .training(model={\"fcnet_hiddens\": [64, 64]})\n",
    "    .evaluation(evaluation_num_workers=1)\n",
    ")\n",
    "\n",
    "algo = config.build()  # 2. build the algorithm,\n",
    "\n",
    "for _ in range(5):\n",
    "    print(algo.train())  # 3. train it,\n",
    "\n",
    "algo.evaluate()  # 4. and evaluate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TRFL\n",
    "\n",
    "https://github.com/deepmind/trfl\n",
    "\n",
    "### Avantages\n",
    "\n",
    "### Inconvénients\n",
    "\n",
    "### Algorithmes disponibles\n",
    "\n",
    "- Nombreux algorithmes (https://github.com/deepmind/trfl/blob/master/docs/index.md#learning-updates)\n",
    "\n",
    "### Intégration, modification, confiance\n",
    "\n",
    "- Construit à partir de Tensorflow\n",
    "- Développé par Deepmind\n",
    "- Documentation bien écrite\n",
    "- Dernier commit il y a 2 ans\n",
    "\n",
    "### Exemple d'utilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import trfl\n",
    "\n",
    "# Q-values for the previous and next timesteps, shape [batch_size, num_actions].\n",
    "q_tm1 = tf.get_variable(\n",
    "    \"q_tm1\", initializer=[[1., 1., 0.], [1., 2., 0.]], dtype=tf.float32)\n",
    "q_t = tf.get_variable(\n",
    "    \"q_t\", initializer=[[0., 1., 0.], [1., 2., 0.]], dtype=tf.float32)\n",
    "\n",
    "# Action indices, discounts and rewards, shape [batch_size].\n",
    "a_tm1 = tf.constant([0, 1], dtype=tf.int32)\n",
    "r_t = tf.constant([1, 1], dtype=tf.float32)\n",
    "pcont_t = tf.constant([0, 1], dtype=tf.float32)  # the discount factor\n",
    "\n",
    "# Q-learning loss, and auxiliary data.\n",
    "loss, q_learning = trfl.qlearning(q_tm1, a_tm1, r_t, pcont_t, q_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ReAgent\n",
    "\n",
    "![ReAgent logo](https://raw.githubusercontent.com/facebookresearch/ReAgent/main/logo/reagent_banner.png \"ReAgent logo\")\n",
    "\n",
    "https://github.com/facebookresearch/ReAgent\n",
    "\n",
    "### Avantages\n",
    "\n",
    "- Algorithmes pour Bandits\n",
    "\n",
    "### Inconvénients\n",
    "\n",
    "- Installation via Docker\n",
    "- Orienté recommandation\n",
    "- Orienté industrie\n",
    "- S'utilise en ligne de commande\n",
    "\n",
    "### Algorithmes disponibles\n",
    "\n",
    "- Nombreux algorithmes (https://github.com/facebookresearch/ReAgent)\n",
    "\n",
    "### Intégration, modification, confiance\n",
    "\n",
    "- Construit à partir de PyTorch\n",
    "- Développé par Facebook\n",
    "- Documentation bien écrite\n",
    "- Dernier commit il y a 1 semaine\n",
    "\n",
    "### Exemple d'utilisation\n",
    "\n",
    "We have set up Click commands to run our RL workflow. The basic usage pattern is\n",
    "\n",
    "    ./reagent/workflow/cli.py run <module.function> <path/to/config>\n",
    "\n",
    "To train a model online with OpenAI Gym, simply run the Click command:\n",
    "\n",
    "    # set the config\n",
    "    export CONFIG=reagent/gym/tests/configs/cartpole/discrete_dqn_cartpole_online.yaml\n",
    "    # train and evaluate model on gym environment\n",
    "    ./reagent/workflow/cli.py run reagent.gym.tests.test_gym.run_test $CONFIG\n",
    "\n",
    "To train a batch RL model, run the following commands:\n",
    "\n",
    "    # set the config\n",
    "    export CONFIG=reagent/workflow/sample_configs/discrete_dqn_cartpole_offline.yaml\n",
    "    # gather some random transitions (can replace with your own)\n",
    "    ./reagent/workflow/cli.py run reagent.workflow.gym_batch_rl.offline_gym_random $CONFIG\n",
    "    # convert data to timeline format\n",
    "    ./reagent/workflow/cli.py run reagent.workflow.gym_batch_rl.timeline_operator $CONFIG\n",
    "    # train model based on timeline data\n",
    "    ./reagent/workflow/cli.py run reagent.workflow.training.identify_and_train_network $CONFIG\n",
    "    # evaluate the model\n",
    "    ./reagent/workflow/cli.py run reagent.workflow.gym_batch_rl.evaluate_gym \"$CONFIG\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. SLM Lab\n",
    "\n",
    "https://slm-lab.gitbook.io/slm-lab/\n",
    "\n",
    "### Avantages\n",
    "\n",
    "### Inconvénients\n",
    "\n",
    "- N'est pas un package python\n",
    "\n",
    "### Algorithmes disponibles\n",
    "\n",
    "- DQN\n",
    "- SARSA\n",
    "- ouble-DQN, Dueling-DQN, PER\n",
    "- A2C (Advantage Actor-Critic) with GAE & n-step\n",
    "- REINFORCE\n",
    "- PPO\n",
    "- SAC\n",
    "- SIL\n",
    "- Asynchronous version of all the above\n",
    "\n",
    "### Intégration, modification, confiance\n",
    "\n",
    "- Documentation bien écrite\n",
    "- Dernier commit il y a 1 an\n",
    "\n",
    "### Exemple d'utilisation\n",
    "\n",
    "https://slm-lab.gitbook.io/slm-lab/setup/installation\n",
    "\n",
    "https://colab.research.google.com/gist/kengz/6fd52a902129fb6d4509c721d71bda48/slm_lab_colab.ipynb#scrollTo=MfrncRH9-j-1\n",
    "\n",
    "    python run_lab.py slm_lab/spec/demo.json dqn_cartpole dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. DeeR\n",
    "\n",
    "https://slm-lab.gitbook.io/slm-lab/\n",
    "\n",
    "### Avantages\n",
    "\n",
    "### Inconvénients\n",
    "\n",
    "### Algorithmes disponibles\n",
    "\n",
    "- Multiple algorithmes (https://deer.readthedocs.io/en/0.4.1/modules/learning-algorithms.html)\n",
    "\n",
    "### Intégration, modification, confiance\n",
    "\n",
    "- Documentation bien écrite\n",
    "- Dernier commit il y a 2 an\n",
    "\n",
    "### Exemple d'utilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from deer.base_classes import Environment\n",
    "import gym\n",
    "\n",
    "class MyEnv(Environment):\n",
    "    def __init__(self, rng):\n",
    "        \"\"\" Initialize environment.\n",
    "        Arguments:\n",
    "            rng - the numpy random number generator            \n",
    "        \"\"\"\n",
    "        # Defining the type of environment\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        self._last_observation = self.env.reset()\n",
    "        self.is_terminal=False\n",
    "        self._input_dim = [(1,), (1,), (1,), (1,)]  # self.env.observation_space.shape is equal to 4 \n",
    "                                                    # and we use only the current observations in the pseudo-state\n",
    "\n",
    "    def act(self, action):\n",
    "        \"\"\" Simulate one time step in the environment.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._last_observation, reward, self.is_terminal, info = self.env.step(action)\n",
    "        if (self.mode==0): # Show the policy only at test time\n",
    "            self.env.render()\n",
    "            \n",
    "        return reward\n",
    "                \n",
    "    def reset(self, mode=0):\n",
    "        \"\"\" Reset environment for a new episode.\n",
    "        Arguments:\n",
    "        Mode : int\n",
    "            -1 corresponds to training and 0 to test\n",
    "        \"\"\"\n",
    "        # Reset initial observation to a random x and theta\n",
    "        self._last_observation = self.env.reset()\n",
    "        self.is_terminal=False\n",
    "        self.mode=mode\n",
    "\n",
    "        return self._last_observation\n",
    "                \n",
    "    def inTerminalState(self):\n",
    "        \"\"\"Tell whether the environment reached a terminal state after the last transition (i.e. the last transition \n",
    "        that occured was terminal).\n",
    "        \"\"\"\n",
    "        return self.is_terminal\n",
    "\n",
    "    def inputDimensions(self):\n",
    "        return self._input_dim  \n",
    "\n",
    "    def nActions(self):\n",
    "        return 2 #Would be useful to have this directly in gym : self.env.action_space.shape  \n",
    "\n",
    "    def observe(self):\n",
    "        return copy.deepcopy(self._last_observation)\n",
    "        \n",
    "def main():\n",
    "    rng = np.random.RandomState(123456)\n",
    "    myenv=MyEnv(rng)\n",
    "\n",
    "    print (myenv.observe())\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\"\"\" Pendulum environment launcher.\n",
    "Same principles as run_toy_env. See the docs for more details.\n",
    "Authors: Vincent Francois-Lavet, David Taralla\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "import deer.experiment.base_controllers as bc\n",
    "from deer.default_parser import process_args\n",
    "from deer.agent import NeuralAgent\n",
    "from deer.learning_algos.q_net_keras import MyQNetwork\n",
    "from pendulum_env import MyEnv as pendulum_env\n",
    "\n",
    "class Defaults:\n",
    "    # ----------------------\n",
    "    # Experiment Parameters\n",
    "    # ----------------------\n",
    "    STEPS_PER_EPOCH = 100\n",
    "    EPOCHS = 200\n",
    "    STEPS_PER_TEST = 100\n",
    "    PERIOD_BTW_SUMMARY_PERFS = 10\n",
    "\n",
    "    # ----------------------\n",
    "    # Environment Parameters\n",
    "    # ----------------------\n",
    "    FRAME_SKIP = 1\n",
    "\n",
    "    # ----------------------\n",
    "    # DQN Agent parameters:\n",
    "    # ----------------------\n",
    "    UPDATE_RULE = 'rmsprop'\n",
    "    LEARNING_RATE = 0.0002\n",
    "    LEARNING_RATE_DECAY = 0.99\n",
    "    DISCOUNT = 0.9\n",
    "    DISCOUNT_INC = 1.\n",
    "    DISCOUNT_MAX = 0.95\n",
    "    RMS_DECAY = 0.9\n",
    "    RMS_EPSILON = 0.0001\n",
    "    MOMENTUM = 0\n",
    "    CLIP_NORM = 1.0\n",
    "    EPSILON_START = 1.0\n",
    "    EPSILON_MIN = 0.2\n",
    "    EPSILON_DECAY = 10000\n",
    "    UPDATE_FREQUENCY = 1\n",
    "    REPLAY_MEMORY_SIZE = 1000000\n",
    "    BATCH_SIZE = 32\n",
    "    FREEZE_INTERVAL = 500\n",
    "    DETERMINISTIC = True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    # --- Parse parameters ---\n",
    "    parameters = process_args(sys.argv[1:], Defaults)\n",
    "    if parameters.deterministic:\n",
    "        rng = np.random.RandomState(12345)\n",
    "    else:\n",
    "        rng = np.random.RandomState()\n",
    "    \n",
    "    # --- Instantiate environment ---\n",
    "    env = pendulum_env(rng)\n",
    "\n",
    "    # --- Instantiate qnetwork ---\n",
    "    qnetwork = MyQNetwork(\n",
    "        env,\n",
    "        parameters.rms_decay,\n",
    "        parameters.rms_epsilon,\n",
    "        parameters.momentum,\n",
    "        parameters.clip_norm,\n",
    "        parameters.freeze_interval,\n",
    "        parameters.batch_size,\n",
    "        parameters.update_rule,\n",
    "        rng,\n",
    "        double_Q=True)\n",
    "    \n",
    "    # --- Instantiate agent ---\n",
    "    agent = NeuralAgent(\n",
    "        env,\n",
    "        qnetwork,\n",
    "        parameters.replay_memory_size,\n",
    "        max(env.inputDimensions()[i][0] for i in range(len(env.inputDimensions()))),\n",
    "        parameters.batch_size,\n",
    "        rng)\n",
    "\n",
    "    # --- Bind controllers to the agent ---\n",
    "    # For comments, please refer to run_toy_env.py\n",
    "    agent.attach(bc.VerboseController(\n",
    "        evaluate_on='epoch', \n",
    "        periodicity=1))\n",
    "\n",
    "    agent.attach(bc.TrainerController(\n",
    "        evaluate_on='action',\n",
    "        periodicity=parameters.update_frequency, \n",
    "        show_episode_avg_V_value=False, \n",
    "        show_avg_Bellman_residual=False))\n",
    "\n",
    "    agent.attach(bc.LearningRateController(\n",
    "        initial_learning_rate=parameters.learning_rate,\n",
    "        learning_rate_decay=parameters.learning_rate_decay,\n",
    "        periodicity=1))\n",
    "\n",
    "    agent.attach(bc.DiscountFactorController(\n",
    "        initial_discount_factor=parameters.discount,\n",
    "        discount_factor_growth=parameters.discount_inc,\n",
    "        discount_factor_max=parameters.discount_max,\n",
    "        periodicity=1))\n",
    "\n",
    "    agent.attach(bc.EpsilonController(\n",
    "        initial_e=parameters.epsilon_start, \n",
    "        e_decays=parameters.epsilon_decay, \n",
    "        e_min=parameters.epsilon_min,\n",
    "        evaluate_on='action', \n",
    "        periodicity=1, \n",
    "        reset_every='none'))\n",
    "\n",
    "    agent.attach(bc.InterleavedTestEpochController(\n",
    "        id=0, \n",
    "        epoch_length=parameters.steps_per_test, \n",
    "        periodicity=1, \n",
    "        show_score=True,\n",
    "        summarize_every=parameters.period_btw_summary_perfs))\n",
    "    \n",
    "    # --- Run the experiment ---\n",
    "    agent.run(parameters.epochs, parameters.steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "a66de4f78c5b37ba5919fae958b18837be1cfa7ecfdc590b6c4e987de5da92a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
